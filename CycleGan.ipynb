{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CycleGan.ipynb","provenance":[],"collapsed_sections":["jM0M-9CFK5q3","vhQSUlxVJcmY"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"EclYqTnt9fiQ","colab_type":"code","colab":{}},"source":["!pip install -q git+https://github.com/tensorflow/examples.git"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lhSsUx9Nyb3t","colab":{}},"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YfIk2es3hJEd","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import tensorflow_datasets as tfds\n","from tensorflow_examples.models.pix2pix import pix2pix\n","\n","import os\n","import time\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","\n","tfds.disable_progress_bar()\n","AUTOTUNE = tf.data.experimental.AUTOTUNE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9YrUbNqGNems","colab_type":"text"},"source":["# ***Input Pipeline:***"]},{"cell_type":"code","metadata":{"id":"kHu4iQn3A7-x","colab_type":"code","colab":{}},"source":["dataset, metadata = tfds.load('cycle_gan/horse2zebra',\n","                              with_info=True, as_supervised=True)\n","\n","train_horses, train_zebras = dataset['trainA'], dataset['trainB']\n","test_horses, test_zebras = dataset['testA'], dataset['testB']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BsRyPu-FA-KJ","colab_type":"code","colab":{}},"source":["BUFFER_SIZE = 1000\n","BATCH_SIZE = 1\n","IMG_WIDTH = 256\n","IMG_HEIGHT = 256"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xz3m-QFnA_LY","colab_type":"code","colab":{}},"source":["def random_crop(image):\n","  cropped_image = tf.image.random_crop(\n","      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n","\n","  return cropped_image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o9zKZCxxBBdW","colab_type":"code","colab":{}},"source":["def random_jitter(image):\n","  # resizing to 286 x 286 x 3\n","  image = tf.image.resize(image, [286, 286],\n","                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","\n","  # randomly cropping to 256 x 256 x 3\n","  image = random_crop(image)\n","\n","  # random mirroring\n","  image = tf.image.random_flip_left_right(image)\n","\n","  return image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"muhR2cgbLKWW","colab":{}},"source":["# normalizing the images to [-1, 1]\n","def normalize(image):\n","  image = tf.cast(image, tf.float32)\n","  image = (image / 127.5) - 1\n","  return image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLA_kzZqBEyY","colab_type":"code","colab":{}},"source":["def preprocess_image_train(image, label):\n","  image = random_jitter(image)\n","  image = normalize(image)\n","  return image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ir_xDEVzBKVL","colab_type":"code","colab":{}},"source":["def preprocess_image_test(image, label):\n","  image = normalize(image)\n","  return image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wZrcMDWdBSKj","colab_type":"code","colab":{}},"source":["train_horses = train_horses.map(\n","    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(\n","    BUFFER_SIZE).batch(1)\n","\n","train_zebras = train_zebras.map(\n","    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(\n","    BUFFER_SIZE).batch(1)\n","\n","test_horses = test_horses.map(\n","    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n","    BUFFER_SIZE).batch(1)\n","\n","test_zebras = test_zebras.map(\n","    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n","    BUFFER_SIZE).batch(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wDO4hCFKBTCe","colab_type":"code","colab":{}},"source":["sample_horse = next(iter(train_horses))\n","sample_zebra = next(iter(train_zebras))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3XPYFlJsLG9_","colab_type":"text"},"source":["# **Generator & Discriminator Defination:**"]},{"cell_type":"code","metadata":{"id":"j3hCGw_iIhPZ","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9akM7Cr2CsbH","colab_type":"text"},"source":["***Our Generator***"]},{"cell_type":"code","metadata":{"id":"iV-lpbMNCrTG","colab_type":"code","colab":{}},"source":["def generator(output_channels, norm_type='batchnorm'):\n","\n","  down_stack = [\n","      downsample(64, 4, norm_type, apply_norm=False),  # (bs, 128, 128, 64)\n","      downsample(128, 4, norm_type),  # (bs, 64, 64, 128)\n","      downsample(256, 4, norm_type),  # (bs, 32, 32, 256)\n","      downsample(512, 4, norm_type),  # (bs, 16, 16, 512)\n","      downsample(512, 4, norm_type),  # (bs, 8, 8, 512)\n","      downsample(512, 4, norm_type),  # (bs, 4, 4, 512)\n","      downsample(512, 4, norm_type),  # (bs, 2, 2, 512)\n","      downsample(512, 4, norm_type),  # (bs, 1, 1, 512)\n","  ]\n","\n","  up_stack = [\n","      upsample(512, 4, norm_type, apply_dropout=True),\n","      upsample(512, 4, norm_type, apply_dropout=True),\n","      upsample(512, 4, norm_type, apply_dropout=True),\n","      upsample(512, 4, norm_type),\n","      upsample(256, 4, norm_type),\n","      upsample(128, 4, norm_type),\n","      upsample(64, 4, norm_type),\n","  ]\n","\n","  initializer = tf.random_normal_initializer(0., 0.02)\n","  last = tf.keras.layers.Conv2DTranspose(\n","      output_channels, 4, strides=2,\n","      padding='same', kernel_initializer=initializer,\n","      activation='tanh')\n","\n","  concat = tf.keras.layers.Concatenate()\n","\n","  inputs = tf.keras.layers.Input(shape=[None, None, 3])\n","  x = inputs\n","\n","  # Downsampling through the model\n","  skips = []\n","  for down in down_stack:\n","    x = down(x)\n","    skips.append(x)\n","\n","  skips = reversed(skips[:-1])\n","\n","  # Upsampling and establishing the skip connections\n","  for up, skip in zip(up_stack, skips):\n","    x = up(x)\n","    x = concat([x, skip])\n","\n","  x = last(x)\n","\n","  return tf.keras.Model(inputs=inputs, outputs=x)\n"," \n","def downsample(filters, size, norm_type='batchnorm', apply_norm=True):\n","  \n","  initializer = tf.random_normal_initializer(0., 0.02)\n","\n","  result = tf.keras.Sequential()\n","  result.add(\n","      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n","                             kernel_initializer=initializer, use_bias=False))\n","\n","  if apply_norm:\n","    if norm_type.lower() == 'batchnorm':\n","      result.add(tf.keras.layers.BatchNormalization())\n","    elif norm_type.lower() == 'instancenorm':\n","      result.add(InstanceNormalization())\n","\n","  result.add(tf.keras.layers.LeakyReLU())\n","\n","  return result\n","  \n","def upsample(filters, size, norm_type='batchnorm', apply_dropout=False):\n","  \n","  initializer = tf.random_normal_initializer(0., 0.02)\n","\n","  result = tf.keras.Sequential()\n","  result.add(\n","      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n","                                      padding='same',\n","                                      kernel_initializer=initializer,\n","                                      use_bias=False))\n","\n","  if norm_type.lower() == 'batchnorm':\n","    result.add(tf.keras.layers.BatchNormalization())\n","  elif norm_type.lower() == 'instancenorm':\n","    result.add(InstanceNormalization())\n","\n","  if apply_dropout:\n","    result.add(tf.keras.layers.Dropout(0.5))\n","\n","  result.add(tf.keras.layers.ReLU())\n","\n","  return result\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eIv0uVwaCPM4","colab_type":"text"},"source":["***Our Discriminator:***"]},{"cell_type":"code","metadata":{"id":"kIw7-lYvCNuF","colab_type":"code","colab":{}},"source":["def discriminator(norm_type='batchnorm', target=True):\n","\n","  initializer = tf.random_normal_initializer(0., 0.02)\n","\n","  inp = tf.keras.layers.Input(shape=[None, None, 3], name='input_image')\n","  x = inp\n","\n","  if target:\n","    tar = tf.keras.layers.Input(shape=[None, None, 3], name='target_image')\n","    x = tf.keras.layers.concatenate([inp, tar])\n","\n","  down1 = downsample(64, 4, norm_type, False)(x)\n","  down2 = downsample(128, 4, norm_type)(down1)\n","  down3 = downsample(256, 4, norm_type)(down2)\n","\n","  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)\n","  conv = tf.keras.layers.Conv2D(\n","      512, 4, strides=1, kernel_initializer=initializer,\n","      use_bias=False)(zero_pad1)\n","\n","  if norm_type.lower() == 'batchnorm':\n","    norm1 = tf.keras.layers.BatchNormalization()(conv)\n","  elif norm_type.lower() == 'instancenorm':\n","    norm1 = InstanceNormalization()(conv)\n","\n","  leaky_relu = tf.keras.layers.LeakyReLU()(norm1)\n","\n","  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)\n","\n","  last = tf.keras.layers.Conv2D(\n","      1, 4, strides=1,\n","      kernel_initializer=initializer)(zero_pad2)\n","  if target:\n","    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n","  else:\n","    return tf.keras.Model(inputs=inp, outputs=last)\n","\n","class InstanceNormalization(tf.keras.layers.Layer):\n","\n","  def __init__(self, epsilon=1e-5):\n","    super(InstanceNormalization, self).__init__()\n","    self.epsilon = epsilon\n","\n","  def build(self, input_shape):\n","    self.scale = self.add_weight(\n","        name='scale',\n","        shape=input_shape[-1:],\n","        initializer=tf.random_normal_initializer(1., 0.02),\n","        trainable=True)\n","\n","    self.offset = self.add_weight(\n","        name='offset',\n","        shape=input_shape[-1:],\n","        initializer='zeros',\n","        trainable=True)\n","\n","  def call(self, x):\n","    mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n","    inv = tf.math.rsqrt(variance + self.epsilon)\n","    normalized = (x - mean) * inv\n","    return self.scale * normalized + self.offset\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6bF22nVDTJk","colab_type":"code","colab":{}},"source":["OUTPUT_CHANNELS = 3\n","\n","generator_g = generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","generator_f = generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","\n","discriminator_x = discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = discriminator(norm_type='instancenorm', target=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4sZp63rE4jP","colab_type":"code","colab":{}},"source":["LAMBDA = 10"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jM0M-9CFK5q3","colab_type":"text"},"source":["# ***Loss Functions:***"]},{"cell_type":"code","metadata":{"id":"G23V31IVE77X","colab_type":"code","colab":{}},"source":["loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YnGm78VIFgY6","colab_type":"text"},"source":["***Discriminator Loss Function:***"]},{"cell_type":"code","metadata":{"id":"LeK93WH3E9G0","colab_type":"code","colab":{}},"source":["def discriminator_loss(real, generated):\n","  real_loss = loss_obj(tf.ones_like(real), real)\n","\n","  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n","\n","  total_disc_loss = real_loss + generated_loss\n","\n","  return total_disc_loss * 0.5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nmo7m_jAFmjK","colab_type":"text"},"source":["***Generator Loss Function***"]},{"cell_type":"code","metadata":{"id":"ofbjZ_yeE_hK","colab_type":"code","colab":{}},"source":["def generator_loss(generated):\n","  return loss_obj(tf.ones_like(generated), generated)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yADxpfLYFu-h","colab_type":"text"},"source":["***Cycle Consistency loss:***"]},{"cell_type":"code","metadata":{"id":"nn2Md31qFC2x","colab_type":"code","colab":{}},"source":["def calc_cycle_loss(real_image, cycled_image):\n","  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n","  \n","  return LAMBDA * loss1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1y7qCoAEFttD","colab_type":"text"},"source":["***Identity Loss***"]},{"cell_type":"code","metadata":{"id":"9VHCaxqiF8W_","colab_type":"code","colab":{}},"source":["def identity_loss(real_image, same_image):\n","  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n","  return LAMBDA * 0.5 * loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pnyVlDKgIz2p","colab_type":"text"},"source":["***All Optimizers:***"]},{"cell_type":"code","metadata":{"id":"GgFhgPabIyqi","colab_type":"code","colab":{}},"source":["generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","\n","discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vhQSUlxVJcmY","colab_type":"text"},"source":["## ***Checkpoint:***"]},{"cell_type":"code","metadata":{"id":"mRIrSc6DJRSp","colab_type":"code","colab":{}},"source":["checkpoint_path = \"./checkpoints/train\"\n","\n","ckpt = tf.train.Checkpoint(generator_g=generator_g,\n","                           generator_f=generator_f,\n","                           discriminator_x=discriminator_x,\n","                           discriminator_y=discriminator_y,\n","                           generator_g_optimizer=generator_g_optimizer,\n","                           generator_f_optimizer=generator_f_optimizer,\n","                           discriminator_x_optimizer=discriminator_x_optimizer,\n","                           discriminator_y_optimizer=discriminator_y_optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","# if a checkpoint exists, restore the latest checkpoint.\n","if ckpt_manager.latest_checkpoint:\n","  ckpt.restore(ckpt_manager.latest_checkpoint)\n","  print ('Latest checkpoint restored!!')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yYEBjLgCJlBE","colab_type":"text"},"source":["# ***Training:***"]},{"cell_type":"code","metadata":{"id":"T7OlaRqHJ1FM","colab_type":"code","colab":{}},"source":["EPOCHS = 37"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SCe7ORuFJ-zj","colab_type":"code","colab":{}},"source":["def generate_images(model, test_input):\n","  prediction = model(test_input)\n","  \n","  plt.figure(figsize=(12, 12))\n","\n","  display_list = [test_input[0], prediction[0]]\n","  title = ['Input Image', 'Predicted Image']\n","\n","  for i in range(2):\n","    plt.subplot(1, 2, i+1)\n","    plt.title(title[i])\n","    # getting the pixel values between [0, 1] to plot it.\n","    plt.imshow(display_list[i] * 0.5 + 0.5)\n","    plt.axis('off')\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2QsxxZ4HKAJv","colab_type":"code","colab":{}},"source":["@tf.function\n","def train_step(real_x, real_y):\n","  with tf.GradientTape(persistent=True) as tape:\n","    \n","    fake_y = generator_g(real_x, training=True)\n","    cycled_x = generator_f(fake_y, training=True)\n","\n","    fake_x = generator_f(real_y, training=True)\n","    cycled_y = generator_g(fake_x, training=True)\n","\n","    # same_x and same_y are used for identity loss.\n","    same_x = generator_f(real_x, training=True)\n","    same_y = generator_g(real_y, training=True)\n","\n","    disc_real_x = discriminator_x(real_x, training=True)\n","    disc_real_y = discriminator_y(real_y, training=True)\n","\n","    disc_fake_x = discriminator_x(fake_x, training=True)\n","    disc_fake_y = discriminator_y(fake_y, training=True)\n","\n","    # calculate the loss\n","    gen_g_loss = generator_loss(disc_fake_y)\n","    gen_f_loss = generator_loss(disc_fake_x)\n","    \n","    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n","    \n","    # Total generator loss = adversarial loss + cycle loss + identity loss\n","    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n","    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n","\n","    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n","    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n","  \n","  # Calculate the gradients for generator and discriminator\n","  generator_g_gradients = tape.gradient(total_gen_g_loss,\n","                                        generator_g.trainable_variables)\n","  generator_f_gradients = tape.gradient(total_gen_f_loss, \n","                                        generator_f.trainable_variables)\n","  \n","  discriminator_x_gradients = tape.gradient(disc_x_loss, \n","                                            discriminator_x.trainable_variables)\n","  discriminator_y_gradients = tape.gradient(disc_y_loss, \n","                                            discriminator_y.trainable_variables)\n","  \n","  # Apply the gradients to the optimizer\n","  generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n","                                            generator_g.trainable_variables))\n","\n","  generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n","                                            generator_f.trainable_variables))\n","  \n","  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n","                                                discriminator_x.trainable_variables))\n","  \n","  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n","                                                discriminator_y.trainable_variables))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rCYBoGhAKdXx","colab_type":"code","colab":{}},"source":["for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_horses, train_zebras)):\n","    train_step(image_x, image_y)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n+=1\n","\n","  clear_output(wait=True)\n","  \n","  generate_images(generator_g, sample_horse)\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3hQBPsKuKl9n","colab_type":"text"},"source":["# ***Testing:***"]},{"cell_type":"code","metadata":{"id":"rSWRMimEKhWi","colab_type":"code","outputId":"6f33c819-8db7-4d87-d76f-d9ad86ff2daf","executionInfo":{"status":"ok","timestamp":1576833060216,"user_tz":-300,"elapsed":9397,"user":{"displayName":"Affan Mehmood","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDRnwmxOQj2Zf5XUnAwSgmlS7hs1WY85o1FzaJa8w=s64","userId":"05658900946857414101"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1lGfcdVYEBGI_SWXSAPzIN5SUfX0FF9b6"}},"source":["# Run the trained model on the test dataset\n","for inp in test_horses.take(15):\n","  generate_images(generator_g, inp)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}